---
title: "lab3"
author: "YonatanLourie-NatiShapiro"
date: "6/2/2021"
output:
  html_document:
    rmarkdown::html_document:
    theme: journal
    toc: true
    toc_depth: 2
    df_print: paged
---
# Lab 3
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

```


```{r, message=FALSE, results==hide, warning=FALSE}
library(tidyverse)
library(sandwich)
library(lmtest)
library(tree)
set.seed(42)
```

## Classification
### 1.
Load the Spam data from the file spam.data using the read.table function
```{r}
dat <- read.table("spam.data")
dat$V58 = as.factor(ifelse(dat$V58 == 1 , "Yes", "No"))
head(dat)
```
We changed the dependent variable to "Yes" and "No" to be a factor for the classification.
We can also see that their are 57 expleainatory variables.

### 2.
Learn your data:
```{r}
dim(dat)
str(dat)
# summary(dat)
```
We can observed that we have only numeric data.
We can also tell that we dont have negative values (with the summary function).

### 3.
Split the data into train and test sets 
```{r}
sample <- sample.int(n = nrow(dat), size = floor(.7*nrow(dat)), replace = F)
train <- dat[sample, ]
test  <- dat[-sample, ]

X_train <- train[,c(1:57)]
X_test <- test[,c(1:57)]

Y_train <- train[,c(58)]
Y_test <- test[,c(58)]
```

### 5. Tree
#### a.
Load the tree library

```{r}
library(tree)
```

#### b.
Fit a tree model using the tree function
```{r}
tree.model <- tree(V58 ~., data=train)
```


#### c. TODO (add expenation)
Print summary of the tree model and examine the results
```{r}
summary(tree.model)
```
We can see that their are 10 explanatory variables that are used in this model. The algorithm worked recursively and partitioned the variables using the connection with the response (via gini). Which means that the largest Gini impurity was for V53.
We can see that the misclassification error rate is 0.09161

#### d. TODO: explanation
Plot the model using plot(model) and text(model, pretty=0), what can you
say about the results?

```{r}
plot(tree.model)
text(tree.model,pretty=0,cex=0.6)

```



#### e.
Use the model for prediction on the test set
```{r}
predictionTreeTest <- predict(tree.model, X_test, type = "class")
summary(predictionTreeTest)
```


#### f.
Evaluate the fit by printing the accuracy and the confusion matrix (recall
that a confusion matrix is the prediction vs the truth)
```{r}
table(predictionTreeTest, actual = Y_test)
accuracy <- mean(predictionTreeTest == Y_test)
accuracy
```
Recall that the main diaognal is for the true fit of the model {(TRUE, TRUE), (FALSE, FALSE)} and what else is our false positive and true negative results.
The accuracy is the porportion of the main diagnal: `r accuracy `


#### g. TODO (not sure)
Recall that we prune the tree in order to avoid overfitting. The tree size is a
tuning parameter and defines the complexity of the tree, thus it is
determined by cross-validation. Run cross-validation to identify how to
prune the tree.

We want to find the tree the minimum deviance:
(Pay attention that FUN=prune.misclass in order to indicate that we want the
classification error rate to guide the cross-validation and pruning process)
```{r}

tree.model.cv = cv.tree(tree.model, FUN=prune.misclass)
# index of tree with minimum error
min_idx = which.min(tree.model.cv$dev)

# number of terminal nodes in that tree
tree.model.cv$size[min_idx]

par(mfrow = c(1, 2))

plot(tree.model.cv)
tree.model.cv$size[min_idx]

points(tree.model.cv$size[min_idx] , tree.model.cv$dev[min_idx], col = "red", cex = 2, pch = 20)
plot(tree.model.cv$size, tree.model.cv$dev / nrow(train), type = "b",
     xlab = "Tree Size", ylab = "CV Misclassification Rate")
points(tree.model.cv$size[min_idx] , tree.model.cv$dev[min_idx], col = "red", cex = 2, pch = 20)


```
So the number of the treminal nodel with the minimum deviance is 10-13

#### h. TODO (not sure)
Examine the output of the cv function (print the output from g). What is
size? What is k?

```{r}
tree.model.cv
```
So the best size is 13 and 10 (we will prefer 10), and the maximum k is 618, We can see that when the k is getting bigger, the deviance is getting bigger as well.
The tuning parameter k controls a trade-off between the subtreeâ€™s complexity and its fit to the training data. When k = 0, then the subtree T will simply equal T, because then it just measures the training error.
However, as l increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity will tend to be minimized for a smaller subtree.
*(The k for size=13 is -inf what the does it mean)*.


#### i 
dev reports the deviance, which is a measure of how good the fit is. Lower
deviance means a better fit.
Plot the deviance as a function of the size and the k.

```{r}
par(mfrow = c(2,1))
plot(tree.model.cv$size, tree.model.cv$dev, type="b", xlab="Tree size", ylab="Deviance")
plot(tree.model.cv$k, tree.model.cv$dev, type="b", xlab="K", ylab="Deviance")

```
As we mentioned above, the best tree size is 10 or 13 (we will prefer 10) with lowest deviance, and the worst k is above ~600.




#### j.
Find the tree size that minimizes the corss-validation error:

We already found it in g. The best tree size is 10.


#### k.
Prune the tree using the value you found in k. by running
prune.misclass(tree_mod, best = best_size)

```{r}
tree.model.cv.prune <- prune.misclass(tree.model, best=10)
plot(tree.model.cv.prune)
text(tree.model.cv.prune , pretty =0, cex=0.7)

```

#### l.  TODO (not sure)
Repeat d.-f. with the pruned tree


```{r}
predictionTreeTest <- predict(tree.model.cv.prune, X_test, type = "class")
summary(predictionTreeTest)
table(predictionTreeTest, actual = Y_test)
accuracy <- mean(predictionTreeTest == Y_test)
accuracy
tree.model.cv = cv.tree(tree.model, FUN=prune.misclass)
# index of tree with minimum error
min_idx = which.min(tree.model.cv$dev)

# number of terminal nodes in that tree
tree.model.cv$size[min_idx]

par(mfrow = c(1, 2))

plot(tree.model.cv)
tree.model.cv$size[min_idx]

points(tree.model.cv$size[min_idx] , tree.model.cv$dev[min_idx], col = "red", cex = 2, pch = 20)
plot(tree.model.cv$size, tree.model.cv$dev / nrow(train), type = "b",
     xlab = "Tree Size", ylab = "CV Misclassification Rate")
points(tree.model.cv$size[min_idx] , tree.model.cv$dev[min_idx], col = "red", cex = 2, pch = 20)
tree.model.cv

par(mfrow = c(2,1))
plot(tree.model.cv$size, tree.model.cv$dev, type="b", xlab="Tree size", ylab="Deviance")
plot(tree.model.cv$k, tree.model.cv$dev, type="b", xlab="K", ylab="Deviance")

```
We got the same values which means that it is the best tree model.

### 6 Random Forest

#### a.
Load randomForset package
```{r}
library(randomForest)
```


#### b.
Fit a random forest model using the randomForest function
```{r}
model6 <- randomForest(V58~., data=train)
model6
```


#### c.
Use the model for prediction on the test set and evaluate the fit by its
accuracy and the confusion matrix.

```{r}
RandomForestP <- predict(model6, newdata = X_test) #predict on the test set

#check accuracy of predictions
table(predicted = RandomForestP, actual = Y_test) 
mean(RandomForestP==Y_test)
```
So our accuracy is `r mean(RandomForestP==Y_test)`.


### 7. Regression trees
#### a-b
Read California Housing data set stored in a CSV file



```{r}
load("CA_samp.Rdata")
df <- data.frame(CA_samp)
sample <- sample.int(n = nrow(df), size = floor(.8*nrow(df)), replace = F)
# df <- lapply(df, function(x) c(scale(x)))
# df[c(1:528)] <- lapply(df[c(1:528)], function(x) c(scale(x)))

train_CA <- df[sample, ]
test_CA  <- df[-sample, ]
train_CA <- data.frame(scale(train_CA))
test_CA  <- data.frame(scale(test_CA))

X_train <- train_CA[, -which(names(train_CA) == "medianHouseValue")]
X_test <- test_CA[, -which(names(test_CA) == "medianHouseValue")]

Y_train <- train_CA[, which(names(train_CA) == "medianHouseValue")]
Y_test <- test_CA[, which(names(test_CA) == "medianHouseValue")]

```


#### c.
Everything that was done for the classification setting can be repeated in
the regression setting in exactly the same way! Fit a regression tree to the
data

```{r}
CA.tree.model <- tree(medianHouseValue~., data=train_CA)
summary(CA.tree.model)
```
We left with 10 variables, and 11 terminal nodes
```{r}
plot(CA.tree.model) 
text(CA.tree.model, pretty=0, cex=0.8)
```


#### d.
Use the model for prediction on the test set and evaluate the fit using the
root mean squared error (RMSE)

```{r}
CA_predictions <- predict(CA.tree.model, newdata = X_test)
sqrt(mean(CA_predictions-Y_test)^2)

```
The RMSE is  `r sqrt(mean(CA_predictions-Y_test)^2)`

But we want to check if we can make it better with cross validation as we did in the last tree model:

```{r}
CA.tree.model.cv <- cv.tree(CA.tree.model)
plot(CA.tree.model.cv$size, sqrt(CA.tree.model.cv$dev / nrow(train)), type = "b",
     xlab = "Tree Size", ylab = "CV-RMSE")
```
```{r}
(CA.tree.model.cv)
```
We cab see that we have the lowest deviance with tree with the size of 11, so we can leave it that way.


### 8. Random Forest

#### a.
Fit a random forest model in exactly the same way, but set
importance=TRUE

#### b.
Use the model for prediction on the test set and evaluate the fit using the
root mean squared error (RMSE)

#### c.
Run importance and varImpPlot on the received model. These are some
measures for importance of variables. For more see ?importance and ILSR
pg. 330.

