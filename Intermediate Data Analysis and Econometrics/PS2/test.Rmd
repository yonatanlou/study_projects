---
title: "Your Document Title"
author: "Document Author"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

{r}
setwd("C:\\Users\\Admin\\Desktop\\Studies\\2nd, 2nd\\Applied econometrics\\PS2")
       
knitr::opts_chunk$set(warning=FALSE)

{r, message=FALSE, results==hide, warning=FALSE}
library(magrittr)
library(haven)
library(Hmisc)
library(tidyverse)
library(ggplot2)
library(magrittr)
library(dplyr)
library(stargazer)
library(plyr)
library(ri)
library(scales)
library(readstata13)
library(gridExtra)
library(sandwich)
library(lmtest)

$$
(2) \ \ E\left(r_{j}\right)-r_{f}=\beta_{j}\left(E\left(r_{M}\right)-r_{f}\right) \ \ \ S.t: \ \ 
\beta_j = \frac{cov(r_j, r_M)}{var(r_M)}
$$

Part 1. CAPM -$\beta$ and Understanding OLS

1.

{r}
df <- read.dta13("data_part1.dta")
df$rf <- 0.0041
df$rM_rf <- df$r_M - df$rf
df$rA_rf <- df$r_A - df$rf
df$rB_rf <- df$r_B - df$rf
head(df)

TODO 

2. sample variance of $(r_A; r_B; r_M)$  , and separately of $(r_A-r_f ; r_B-r_f ; r_M-r_f )$

{r}
a = lapply(df[c("r_A", "r_B", "r_M")], var)
b = lapply(df[c("rA_rf", "rB_rf", "rM_rf")], var)

var_diff = data.frame(Variance=unlist(a),DiffVar=unlist(b))
var_diff

#or maybe should be:
#cov(df[c("r_A", "r_B", "r_M")])
#cov(df[c("rA_rf", "rB_rf", "rM_rf")])

We can see the tow sample variance is exactly the same - and that because $r_f$ is constant and we know that $Var(X +c) = Var(X)$

3. Scatterplots - SPDR VS. MS

{r}
y.lim <- c(min(df$rA_rf,df$rB_rf), max(df$rA_rf,df$rB_rf))
plot1 <- ggplot(df, aes(x=rM_rf, y=rA_rf)) + geom_point() + ylim(y.lim) +ggtitle("SPDR Gold Shares")
plot2 <- ggplot(df, aes(x=rM_rf, y=rB_rf)) + geom_point() + ylim(y.lim) +ggtitle("Morgan Stanely")
grid.arrange(plot1, plot2, ncol=2) 

4. (a)Linear regression:

consider: $\left(r_{j t}-r_{f}\right)=\alpha_{j}+\beta_{j}\left(r_{M t}-r_{f}\right)+\epsilon_{j t}$ and ill prove that $\beta_j = \frac{cov(r_j, r_M)}{var(r_M)}$

let $Y=(r_A-r_f), X=(r_M-r_f)$ I will derive to projection with the projection formula $(X^TX)^{-1}X^TY_j$.

{r}
#define our Y, X
Y_1 <- df$rA_rf
Y_2 <- df$rB_rf
X <- df$rM_rf
intercept <- rep(1, length(Y_1)) #making the p=0
X <- cbind(intercept, X)

r_A <- df$r_A
r_B <- df$r_B
r_M <- df$r_M

#projection of X on Y
coeff1 <- solve(t(X) %*% X) %*% t(X) %*% Y_1
reg.beta1 <- round(coeff1[2],3)
cov.r_A <- round(cov(r_A, r_M)/var(r_M),3)


coeff2 <- solve(t(X) %*% X) %*% t(X) %*% Y_2
reg.beta2 <- round(coeff2[2],3)
cov.r_B <- round(cov(r_B, r_M)/var(r_M),3)

print(paste0("For beta_A: ",cov.r_A, " = ", reg.beta1))
print(paste0("For beta_B: ",cov.r_B, " = ", reg.beta2))

(b) TODO

Easy to see that for $E\left(r_{j}\right)-r_{f}=\beta_{j}\left(E\left(r_{M}\right)-r_{f}\right)$ when $Y=(r_A-r_f), X=(r_M-r_f)$ and we know that $E\left(r_{j}\right)-r_{f}=\beta_{j}\left(E\left(r_{M}\right)-r_{f}\right) \rightarrow E(E\left(r_{j}\right)-r_{f})=E(\beta_{j}\left(E\left(r_{M}\right)-r_{f}\right)) \rightarrow E(Y) = \beta_jE(X)$

(c) Estimate equation 2 by OLS regression separately for gold and Morgan Stanley shares using the lm function

{r}
Y_1 <- df$rA_rf
Y_2 <- df$rB_rf
X <- df$rM_rf

r_A <- df$r_A
r_B <- df$r_B
r_M <- df$r_M


reg.model1 <- lm(Y_1~X)
reg.beta1 <- round(reg.model1$coefficients[2],3)
reg.alpha1 <- reg.model1$coefficients[1]

reg.model2 <- lm(Y_2~X)
reg.beta2 <- round(reg.model2$coefficients[2],3)
reg.alpha2 <- reg.model2$coefficients[1]


My estimated values $\hat\beta_A =$ r reg.beta1 , $\hat\beta_B =$ r reg.beta2

TODO

Not extacly because its not the same formula, but we can observe that in both cases $\beta_A$ is smaller than $\beta_B$

Because the beta of SPDR is really low (closer to zero) it means that it less correlated with the market the MS (for example, for every rise of 1000$ in the market, SPDR will be r 1000*reg.beta1 and MS will be r 1000*1.683 .

{r}
y.lim <- c(min(df$rA_rf,df$rB_rf), max(df$rA_rf,df$rB_rf))
plot1 <- ggplot(df, aes(x=rM_rf, y=rA_rf)) + geom_point()+geom_smooth(method = "lm", se = FALSE) + ylim(y.lim) +ggtitle("SPDR Gold Shares")
plot2 <- ggplot(df, aes(x=rM_rf, y=rB_rf)) + geom_point() +geom_smooth(method = "lm", se = FALSE) + ylim(y.lim) +ggtitle("Morgan Stanely")
grid.arrange(plot1, plot2, ncol=2) 

TODO

The worried investor should

Part 2. CAPM -Multiple Hypothesis Testing and the Search for $\alpha$

1

{r}
df2 <- read.dta13("data_part2.dta")
df2_len <- length(colnames(df2))

df2[2:df2_len] <- lapply(df2[2:df2_len], function(x) x-0.0041)

2

{r}
SQPX.reg <- lm(r_swppx ~ r_M, data=df2)
SQPX.alpha <- SQPX.reg$coefficients[1]
SQPX.beta <- SQPX.reg$coefficients[2]
print(paste0("alpha is: ", round(SQPX.alpha,4), " beta is: ",round(SQPX.beta,4)))

TODO

(a) In equation 2, the variance of the residulas may vary by each year. We can use Homoscedasticity if the variance of the residuals is distinct, and we cant assume that so Homoscedasticity is not plausible.

(b)  Let's test the null hypothesis that $\alpha=0$ with 0.1 significant level (using lmtest and sandwich libraries):

{r}
#vcovHC for heteroscedastic-robust
SQPX.alpha.hat <- SQPX.alpha
SQPX.alpha.se <- sqrt(vcovHC(SQPX.reg)[1,1])
test.stat <- abs(SQPX.alpha.hat/SQPX.alpha.se)
SQPX.alpha.pval <- 2*(1-pt(test.stat,dim(df2)[1]-1))

#Will we reject the null hypothesis at 0.1 level?
test.stat > qt(0.95, dim(df2)[1]-1)
SQPX.alpha.pval <0.1

So we reject the null hypothesis at 0.1 level that $\alpha =0$

(c)

{r}
SQPX.CI <- coefci(SQPX.reg, df = dim(df2)[1]-1, level=.90, vcov = vcovHC)[1,]
SQPX.CI 
#denote that vcovHC for heteroscedastic-robust

(d) We can conclude that the alpha will be greater than 0, but not significtly - in 90% canches the alpha will be between r SQPX.CI[1]  and r SQPX.CI[2]  which is nice but not outperform the market by alot.

3

{r}
# add documntation
Hypo.data <- function(df, fund, r_M, a, fund_name) {
  fund.reg <- lm(fund ~ r_M)
  fund.alpha.hat <- fund.reg$coefficients[1]
  fund.beta.hat <- fund.reg$coefficients[2]
  fund.alpha.se <- sqrt(vcovHC(fund.reg)[1,1])
  
  test.stat <- abs(fund.alpha.hat/fund.alpha.se)
  fund.alpha.pval <- 2*(1-pt(test.stat,dim(df)[1]-1))
  null_status <- test.stat > qt(1-(a/2), dim(df)[1]-1)
  # fund.alpha.pval < a
  
  fund.CI <- coefci(fund.reg, df = dim(df)[1]-1, level=1-a, vcov = vcovHC)[1,]
  ans <- data.frame(fund = fund_name, alpha= round(fund.alpha.hat,4), beta= round(fund.beta.hat,4), nullHypo = null_status, pval = round(fund.alpha.pval,4))
  return(ans)
}



{r}
## add documtatton
data <- data.frame(fund = NA, alpha= NA, beta= NA, nullHypo = NA, pval = NA)
for (col in c(1:10)) {
  tmpData <- (Hypo.data(df2,df2[,col+2], df2$r_M, 0.1, names(df2)[3:12][col]))
  data <- rbind(data, tmpData)
} 
rownames(data) <- NULL
data <- na.omit(data)
data

We will reject 3 null hypothesis of there funds: vbisx, swtsx, swppx.

Under the CAPM model,  if $\alpha_j>0$ , the asset has a higher expected return than than the market. So i can conclude that the alpha will be the indicator for returns (bigger alpha => bigger returns).

The funds that make me suprised whom made to the list for Time's 50 Best Mutual Funds in 2018 are those with the negative alpha AND with with rejection of the null hypothesis  - vbisx.

TODO - explain why is could equal.

4 

(a) Because we have 10 hypothesis testing, and we checking with a significance level of 0.1, if only one null hypothesis will be True, then we have proportion of $\frac{1}{10}$ True null hypothesis which is 0.1.

(b) If the tow null hypothesis are indipendent $P(A\cap B)=P(A)P(B)$ then we can say that:

$$
P(reject \ any\ null) = 1-P(reject \ no \ null) \\= P(don't \ reject \ H_0^{(1)} \ \& \ \ don't \ reject \ H_0^{(2)})=1-(1-\alpha)^2= 
$$

r 1-(1-0.1)^2   which is indeed greater than 0.1.

(c) According to the proof I showed in the previous section:

$$
P(reject \ any\ null) = 1-P(reject \ no \ null) \\= P(don't \ reject \ H_0^{(1)} \ \& \dots\& \ \ don't \ reject \ H_0^{(n)})=1-(1-\alpha)^n= 1-(1-0.1)^{10} \\ = 0.65
$$

5

{r}
data$bonferon <- data$pval < (0.1/10)
data

We will reject only one null hypothesis (vbisx fund).

6

{r}
alpha= 0.1
data <- arrange(data, pval)
data$k <- c(1:nrow(data))
data$RejHypK <- ifelse((alpha/(10+1-data$k))>data$pval, TRUE, FALSE)
#for k+2 the process is ending.
data

Qusestions to Hannah

1.2 Do i need to compute the variance of each of the variables or the covariance of all of them together (var(A), var(B)…. or cov(A,B,…))

